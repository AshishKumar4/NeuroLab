{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import orbax.checkpoint\n",
    "from flax.metrics import tensorboard\n",
    "from flax.training import train_state\n",
    "import tensorflow_datasets as tfds\n",
    "import optax\n",
    "from typing import Any, Tuple, Mapping,Callable,List,Dict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TrainState with EMA parameters\n",
    "class TrainState(train_state.TrainState):\n",
    "    rngs: jax.random.PRNGKey\n",
    "    ema_params: dict\n",
    "\n",
    "    def get_random_key(self):\n",
    "        rngs, subkey = jax.random.split(self.rngs)\n",
    "        return self.replace(rngs=rngs), subkey\n",
    "\n",
    "    def apply_ema(self, decay: float=0.999):\n",
    "        new_ema_params = jax.tree_util.tree_map(\n",
    "            lambda ema, param: decay * ema + (1 - decay) * param,\n",
    "            self.ema_params,\n",
    "            self.params,\n",
    "        )\n",
    "        return self.replace(ema_params=new_ema_params)\n",
    "\n",
    "class SimpleTrainer:\n",
    "    state : TrainState\n",
    "    best_state : TrainState\n",
    "    best_loss : float\n",
    "    model : nn.Module\n",
    "    ema_decay:float = 0.999\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model:nn.Module, \n",
    "                input_shapes:Dict[str, Tuple[int]],\n",
    "                 optimizer: optax.GradientTransformation,\n",
    "                 rngs:jax.random.PRNGKey,\n",
    "                 train_state:TrainState=None,\n",
    "                 name:str=\"Simple\",\n",
    "                 load_from_checkpoint:bool=False,\n",
    "                 checkpoint_suffix:str=\"\",\n",
    "                 loss_fn=optax.l2_loss,\n",
    "                 param_transforms:Callable=None,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.name = name\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "        options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=4, create=True)\n",
    "        self.checkpointer = orbax.checkpoint.CheckpointManager(self.checkpoint_path() + checkpoint_suffix, checkpointer, options)\n",
    "\n",
    "        if load_from_checkpoint:\n",
    "            latest_step, old_state, old_best_state = self.load()\n",
    "        else:\n",
    "            latest_step, old_state, old_best_state = 0, None, None\n",
    "            \n",
    "        self.latest_step = latest_step\n",
    "\n",
    "        if train_state == None:\n",
    "            self.init_state(input_shapes, optimizer, rngs, existing_state=old_state, existing_best_state=old_best_state, model=model, param_transforms=param_transforms)\n",
    "        else:\n",
    "            self.state = train_state\n",
    "            self.best_state = train_state\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "    def init_state(self,\n",
    "                   input_shapes:Dict[str, Tuple[int]],\n",
    "                   optimizer: optax.GradientTransformation, \n",
    "                   rngs:jax.random.PRNGKey,\n",
    "                   existing_state:dict=None,\n",
    "                   existing_best_state:dict=None,\n",
    "                   model:nn.Module=None,\n",
    "                   param_transforms:Callable=None\n",
    "                   ):\n",
    "        rngs, subkey = jax.random.split(rngs)\n",
    "\n",
    "        if existing_state == None:\n",
    "            input_vars = {k:jnp.ones(v) for k,v in input_shapes.items()}\n",
    "            params = model.init(subkey, **input_vars)\n",
    "            existing_state = {\"params\":params, \"ema_params\":params}\n",
    "\n",
    "        if param_transforms is not None:\n",
    "            params = param_transforms(params)\n",
    "            \n",
    "        self.best_loss = 1e9\n",
    "        self.state = TrainState.create(\n",
    "            apply_fn=model.apply,\n",
    "            params=existing_state['params'],\n",
    "            ema_params=existing_state['ema_params'],\n",
    "            tx=optimizer,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        if existing_best_state is not None:\n",
    "            self.best_state = self.state.replace(params=existing_best_state['params'], ema_params=existing_best_state['ema_params'])\n",
    "        else:\n",
    "            self.best_state = self.state\n",
    "\n",
    "    def checkpoint_path(self):\n",
    "        experiment_name = self.name\n",
    "        path = os.path.join(os.path.abspath('./checkpoints'), experiment_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def load(self):\n",
    "        step = self.checkpointer.latest_step()\n",
    "        print(\"Loading model from checkpoint\", step)\n",
    "        ckpt = self.checkpointer.restore(step)\n",
    "        state = ckpt['state']\n",
    "        best_state = ckpt['best_state']\n",
    "        # Convert the state to a TrainState\n",
    "        self.best_loss = ckpt['best_loss']\n",
    "        print(f\"Loaded model from checkpoint at step {step}\", ckpt['best_loss'])\n",
    "        return step, state, best_state\n",
    "\n",
    "    def save(self, epoch=0):\n",
    "        print(f\"Saving model at epoch {epoch}\")\n",
    "        # filename = os.path.join(self.checkpoint_path(), f'model_{epoch}' if not best else 'best_model')\n",
    "        ckpt = {\n",
    "            'model': self.model,\n",
    "            'state': self.state,\n",
    "            'best_state': self.best_state,\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "        self.checkpointer.save(epoch, ckpt, save_kwargs={'save_args': save_args}, force=True)\n",
    "\n",
    "    def summary(self):\n",
    "        inp = jnp.ones((1, self.image_size, self.image_size, 3))\n",
    "        temb = jnp.ones((1,))\n",
    "        textcontext = jnp.ones((1, 12, 768))\n",
    "        print(self.model.tabulate(jax.random.key(0), inp, temb, textcontext, console_kwargs={\"width\": 200, \"force_jupyter\":True, }))\n",
    "\n",
    "    def _define_train_step(self, batch_size, null_labels_seq):\n",
    "        noise_schedule = self.noise_schedule\n",
    "        model = self.model\n",
    "        model_output_transform = self.model_output_transform\n",
    "        loss_fn = self.loss_fn\n",
    "        unconditional_prob = self.unconditional_prob\n",
    "        \n",
    "        # Determine the number of unconditional samples\n",
    "        num_unconditional = int(batch_size * unconditional_prob)\n",
    "        \n",
    "        nS, nC = null_labels_seq.shape\n",
    "        null_labels_seq = jnp.broadcast_to(null_labels_seq, (batch_size, nS, nC))\n",
    "        \n",
    "        @jax.jit\n",
    "        def train_step(state:TrainState, batch):\n",
    "            \"\"\"Train for a single step.\"\"\"\n",
    "            images = batch['image']\n",
    "            label_seq = batch['label_seq']\n",
    "            \n",
    "            # Generate random probabilities to decide how much of this batch will be unconditional\n",
    "            # state, rngs = state.get_random_key()\n",
    "            # random_prob = jax.random.uniform(rngs, (batch_size,), dtype=jnp.float16)\n",
    "            # is_unconditional = random_prob < unconditional_prob\n",
    "            # # Replace label_seq with null_labels_seq based on the probability\n",
    "            # label_seq = jax.lax.select(is_unconditional[:, None, None], null_labels_seq, label_seq)\n",
    "            \n",
    "            label_seq = jnp.concat([null_labels_seq[:num_unconditional], label_seq[num_unconditional:]], axis=0)\n",
    "\n",
    "            noise_level, state = noise_schedule.generate_timesteps(images.shape[0], state)\n",
    "            state, rngs = state.get_random_key()\n",
    "            noise:jax.Array = jax.random.normal(rngs, shape=images.shape)\n",
    "            rates = noise_schedule.get_rates(noise_level)\n",
    "            noisy_images, c_in, expected_output = model_output_transform.forward_diffusion(images, noise, rates)\n",
    "            def model_loss(params):\n",
    "                preds = model.apply(params, *noise_schedule.transform_inputs(noisy_images*c_in, noise_level), label_seq)\n",
    "                preds = model_output_transform.pred_transform(noisy_images, preds, rates)\n",
    "                nloss = loss_fn(preds, expected_output)\n",
    "                # nloss = jnp.mean(nloss, axis=1)\n",
    "                nloss *= noise_schedule.get_weights(noise_level)\n",
    "                nloss = jnp.mean(nloss)\n",
    "                loss = nloss\n",
    "                return loss\n",
    "            loss, grads = jax.value_and_grad(model_loss)(state.params)\n",
    "            state = state.apply_gradients(grads=grads) \n",
    "            state = state.apply_ema(self.ema_decay)\n",
    "            return state, loss\n",
    "        return train_step\n",
    "    \n",
    "    def _define_compute_metrics(self):\n",
    "        @jax.jit\n",
    "        def compute_metrics(state:TrainState, expected, pred):\n",
    "            loss = jnp.mean(jnp.square(pred - expected))\n",
    "            metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "            metrics = state.metrics.merge(metric_updates)\n",
    "            state = state.replace(metrics=metrics)\n",
    "            return state\n",
    "        return compute_metrics\n",
    "\n",
    "    def fit(self, data, steps_per_epoch, epochs):\n",
    "        loader = data['loader']\n",
    "        null_labels_full = data['null_labels_full']\n",
    "        batch_size = data['batch_size']\n",
    "        loader = iter(loader)\n",
    "        train_step = self._define_train_step(batch_size, null_labels_full)\n",
    "        compute_metrics = self._define_compute_metrics()\n",
    "        state = self.state\n",
    "        for epoch in range(epochs):\n",
    "            current_epoch = self.latest_step + epoch + 1\n",
    "            print(f\"\\nEpoch {current_epoch}/{epochs}\")\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "            with tqdm.tqdm(total=steps_per_epoch, desc=f'\\t\\tEpoch {current_epoch}', ncols=100, unit='step') as pbar:\n",
    "                for i in range(steps_per_epoch):\n",
    "                    batch = next(loader)\n",
    "                    state, loss = train_step(state, batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i % 100 == 0:\n",
    "                        pbar.set_postfix(loss=f'{loss:.4f}')\n",
    "                        pbar.update(100)\n",
    "            end_time = time.time()\n",
    "            self.state = state\n",
    "            total_time = end_time - start_time\n",
    "            avg_time_per_step = total_time / steps_per_epoch\n",
    "            avg_loss = epoch_loss / steps_per_epoch\n",
    "            if avg_loss < self.best_loss:\n",
    "                self.best_loss = avg_loss\n",
    "                self.best_state = state\n",
    "                self.save(current_epoch)\n",
    "            print(f\"\\n\\tEpoch {current_epoch} completed. Avg Loss: {avg_loss}, Time: {total_time:.2f}s, Best Loss: {self.best_loss}\")\n",
    "        self.save(epochs)\n",
    "        return self.state\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
